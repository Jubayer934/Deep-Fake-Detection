{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jubayer934/Deep-Fake-Detection/blob/main/Hybrid_XcepVit1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7grUKoURZbH"
      },
      "source": [
        "## Connecting Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IynRZzngRYQy",
        "outputId": "b97e6e86-7f23-4714-9de7-837cb49f6fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "OHzruRmAUVST",
        "outputId": "b7013ace-68d0-4fb5-925b-340001731322"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_notebook.ipynb'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4088498110.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_notebook.ipynb'"
          ]
        }
      ],
      "source": [
        "import nbformat\n",
        "\n",
        "# Path to your current notebook file\n",
        "input_path = \"/content/drive/MyDrive/ModelTrain/DFD/Colab/Hybrid_XcepVit.ipynb\"\n",
        "output_path = \"/content/drive/MyDrive/ModelTrain/DFD/Colab/Hybrid_XcepVit1.ipynb\"\n",
        "\n",
        "# Load the notebook\n",
        "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = nbformat.read(f, as_version=4)\n",
        "\n",
        "# Remove the problematic widget metadata\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    nbformat.write(nb, f)\n",
        "\n",
        "print(\"Cleaned notebook saved as:\", output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBB5r0kiRk_t"
      },
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Path to the zip file in Drive\n",
        "# zip_path = \"/content/drive/MyDrive/ModelTrain/DFD/Dataset/DFD.zip\"\n",
        "\n",
        "# # Unzip it to /content/\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"/content/deepfake_dataset\")\n",
        "\n",
        "# # Set dataset path\n",
        "# dataset_path = \"/content/deepfake_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1n9nhTQ4pXe"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file in Drive\n",
        "zip_path = \"/content/drive/MyDrive/ModelTrain/DFD/Dataset/extract_frames.zip\"\n",
        "\n",
        "# Unzip it to /content/\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/deepfake_dataset\")\n",
        "\n",
        "# Set dataset path\n",
        "dataset_path = \"/content/deepfake_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UccPQslRmDi"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "\n",
        "# # List top-level files/folders\n",
        "# print(os.listdir(\"/content/deepfake_dataset\"))\n",
        "\n",
        "# # If it's nested, explore further:\n",
        "# for root, dirs, files in os.walk(\"/content/deepfake_dataset\"):\n",
        "#     print(\"Root:\", root)\n",
        "#     print(\"Dirs:\", dirs)\n",
        "#     print(\"Files:\", files[:5])  # just print the first 5 files\n",
        "#     print(\"===\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0f7f995"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHtbdAgzXSVu"
      },
      "outputs": [],
      "source": [
        "# import cv2\n",
        "# import os\n",
        "\n",
        "# # Function to count readable videos in a directory\n",
        "# def count_readable_videos(directory):\n",
        "#     video_files = [f for f in os.listdir(directory) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
        "#     readable_count = 0\n",
        "#     for video_file in video_files:\n",
        "#         video_path = os.path.join(directory, video_file)\n",
        "#         cap = cv2.VideoCapture(video_path)\n",
        "#         if cap.isOpened():\n",
        "#             readable_count += 1\n",
        "#             cap.release()\n",
        "#     return readable_count\n",
        "\n",
        "# # Count readable videos in each directory\n",
        "# real_videos_dir = \"/content/deepfake_dataset/DFD/DFD_original sequences\"\n",
        "# manipulated_videos_dir = \"/content/deepfake_dataset/DFD/DFD_manipulated_sequences\"\n",
        "\n",
        "# real_readable_count = count_readable_videos(real_videos_dir)\n",
        "# manipulated_readable_count = count_readable_videos(manipulated_videos_dir)\n",
        "\n",
        "# print(f\"Number of readable real videos: {real_readable_count}\")\n",
        "# print(f\"Number of readable manipulated videos: {manipulated_readable_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDAb2i4GQ29Z"
      },
      "source": [
        "## Step 1: Install Required Libraries\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVoSitmZTAdu"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFiy1DaOL_4o"
      },
      "outputs": [],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB08Djt9YMxY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import timm\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B53R9i3dT91y"
      },
      "source": [
        "## Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbFzIW91T88C"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Print the device being used\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtrlpLkFM4fe"
      },
      "source": [
        "## Extracted Frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sa57AHiUgZP"
      },
      "outputs": [],
      "source": [
        "# # Directories for real and manipulated videos\n",
        "# real_videos_dir = \"/content/deepfake_dataset/DFD/DFD_original sequences\"\n",
        "# manipulated_videos_dir = \"/content/deepfake_dataset/DFD/DFD_manipulated_sequences\"\n",
        "\n",
        "# # Output directories for extracted frames\n",
        "# output_real_dir = \"/content/deepfake_dataset/extract_frames/real\"\n",
        "# output_manipulated_dir = \"/content/deepfake_dataset/extract_frames/manipulated\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R70-VdmSNXKf"
      },
      "outputs": [],
      "source": [
        "# # Ensure output directories exist\n",
        "# os.makedirs(output_real_dir, exist_ok=True)\n",
        "# os.makedirs(output_manipulated_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svFh074TUqSy"
      },
      "source": [
        "##  Extract Frames from video\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgSK8sDhmiEM"
      },
      "outputs": [],
      "source": [
        "# def extract_frames_from_videos(videos_dir, output_dir, label, interval_sec=2):\n",
        "#     video_files = [f for f in os.listdir(videos_dir) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
        "#     total_videos = len(video_files)\n",
        "\n",
        "#     for idx, video_file in enumerate(video_files):\n",
        "#         idx += 1\n",
        "#         if idx == 1:\n",
        "#           print(f\"[{idx}/{total_videos}] Processing: {video_file}\")\n",
        "#         elif idx%2==0:\n",
        "#           print(f\"[{idx}/{total_videos}] Processing: {video_file}\")\n",
        "#         elif idx == total_videos:\n",
        "#           print(f\"[{idx}/{total_videos}] Processing: {video_file}\")\n",
        "\n",
        "#         video_path = os.path.join(videos_dir, video_file)\n",
        "#         cap = cv2.VideoCapture(video_path)\n",
        "#         fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "#         total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "#         if fps <= 0:\n",
        "#             print(f\"‚ö†Ô∏è Skipping {video_file} (FPS not detected)\")\n",
        "#             continue\n",
        "\n",
        "#         # Step size in frames\n",
        "#         step = fps * interval_sec\n",
        "\n",
        "#         frame_num = 0\n",
        "#         while frame_num < total_frames:\n",
        "#             cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)  # jump directly\n",
        "#             success, image = cap.read()\n",
        "#             if not success:\n",
        "#                 break\n",
        "\n",
        "#             frame_filename = f\"{label}_{video_file}_frame{frame_num // step}.jpg\"\n",
        "#             frame_path = os.path.join(output_dir, frame_filename)\n",
        "#             cv2.imwrite(frame_path, image)\n",
        "\n",
        "#             frame_num += step  # jump to next interval\n",
        "\n",
        "#         cap.release()\n",
        "\n",
        "#     print(f\"‚úÖ Finished extracting frames every {interval_sec}s from {label} videos.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgfac5-nUnGF"
      },
      "outputs": [],
      "source": [
        "# def extract_frames_from_videos_with_every_fps(videos_dir, output_dir, label):\n",
        "#     video_files = [f for f in os.listdir(videos_dir) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
        "#     total_videos = len(video_files)\n",
        "\n",
        "#     for idx, video_file in enumerate(video_files):\n",
        "#         print(f\"[{idx + 1}/{total_videos}] Processing: {video_file}\")\n",
        "\n",
        "#         video_path = os.path.join(videos_dir, video_file)\n",
        "#         cap = cv2.VideoCapture(video_path)\n",
        "#         frame_count = 0\n",
        "#         success, image = cap.read()\n",
        "\n",
        "#         while success:\n",
        "#             if frame_count % int(cap.get(cv2.CAP_PROP_FPS)) == 0:\n",
        "#                 frame_filename = f\"{label}_{video_file}_frame{frame_count // int(cap.get(cv2.CAP_PROP_FPS))}.jpg\"\n",
        "#                 frame_path = os.path.join(output_dir, frame_filename)\n",
        "#                 cv2.imwrite(frame_path, image)\n",
        "#             success, image = cap.read()\n",
        "#             frame_count += 1\n",
        "\n",
        "#         cap.release()\n",
        "\n",
        "#     print(f\"‚úÖ Finished extracting frames from {label} videos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhM4ziWBUyOB"
      },
      "outputs": [],
      "source": [
        "# extract_frames_from_videos(real_videos_dir, output_real_dir, \"real\")\n",
        "# extract_frames_from_videos(manipulated_videos_dir, output_manipulated_dir, \"manipulated\")\n",
        "# print(\"Frame extraction completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnnQOiQpW2AE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "real_frames_dir = \"/content/deepfake_dataset/extract_frames/real\"\n",
        "num_real_frames = len([f for f in os.listdir(real_frames_dir) if os.path.isfile(os.path.join(real_frames_dir, f))])\n",
        "print(f\"Number of frames in {real_frames_dir}: {num_real_frames}\")\n",
        "\n",
        "manipulated_dir = \"/content/deepfake_dataset/extract_frames/manipulated\"\n",
        "num_manipulated_frames = len([f for f in os.listdir(manipulated_dir) if os.path.isfile(os.path.join(manipulated_dir, f))])\n",
        "print(f\"Number of frames in {manipulated_dir}: {num_manipulated_frames}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UInKiUPZVPIK"
      },
      "source": [
        "## Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9a9uEbaRxf8"
      },
      "outputs": [],
      "source": [
        "# Define image transformations with advanced augmentations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEQ2SNEKVYRb"
      },
      "source": [
        "## Train and Validation Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d3H4kbbFkE1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import multiprocessing\n",
        "import torch\n",
        "\n",
        "def system_info():\n",
        "    # CPU cores\n",
        "    cpu_count = multiprocessing.cpu_count()\n",
        "    print(f\"Total CPU cores: {cpu_count}\")\n",
        "\n",
        "    # GPU info (if PyTorch + CUDA available)\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_count = torch.cuda.device_count()\n",
        "        print(f\"Total GPUs: {gpu_count}\")\n",
        "        for i in range(gpu_count):\n",
        "            print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    else:\n",
        "        print(\"No GPU detected.\")\n",
        "\n",
        "    # Recommended DataLoader workers\n",
        "    # Rule of thumb: num_workers = number of CPU cores // number of GPUs (if GPUs exist), or just half of CPU cores\n",
        "    if torch.cuda.is_available():\n",
        "        recommended_workers = max(1, cpu_count // torch.cuda.device_count())\n",
        "    else:\n",
        "        recommended_workers = max(1, cpu_count // 2)\n",
        "\n",
        "    print(f\"Recommended DataLoader workers: {recommended_workers}\")\n",
        "\n",
        "system_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQPOBGequfG4"
      },
      "outputs": [],
      "source": [
        "dataset_dir = \"/content/deepfake_dataset/extract_frames\"  # Directory where frames are saved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlNuw8j9BqeV"
      },
      "source": [
        "# --------------------------\n",
        "# Set random seeds for reproducibility\n",
        "# --------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEvphbj7AVBs"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Subset, DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import datasets\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBlm5U06CDZy"
      },
      "source": [
        "# --------------------------\n",
        "# Load dataset\n",
        "# --------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhEV61EgAtbR"
      },
      "outputs": [],
      "source": [
        "full_dataset = datasets.ImageFolder(root=dataset_dir, transform=transform_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqwQBnJeAzqp"
      },
      "outputs": [],
      "source": [
        "# Extract indices and labels for stratified split\n",
        "indices = list(range(len(full_dataset)))\n",
        "labels = full_dataset.targets  # labels from ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnooz8o9A2ZW"
      },
      "outputs": [],
      "source": [
        "# Train/Validation split (stratified, reproducible)\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices,\n",
        "    test_size=0.2,\n",
        "    stratify=labels,\n",
        "    random_state=seed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvT29qoXA6mK"
      },
      "outputs": [],
      "source": [
        "# Subsets\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "val_dataset   = Subset(full_dataset, val_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAfOYu5CA-fZ"
      },
      "outputs": [],
      "source": [
        "# Override validation dataset transform\n",
        "val_dataset.dataset.transform = transform_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLx8GaJoBFvn"
      },
      "source": [
        "# --------------------------\n",
        "# Oversampling on training set\n",
        "# --------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h-8IKd7BK6X"
      },
      "outputs": [],
      "source": [
        "train_labels = [full_dataset.targets[i] for i in train_idx]\n",
        "train_labels = torch.tensor(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUG96DtFzFM-"
      },
      "outputs": [],
      "source": [
        "print(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-E8eXd6BOh_"
      },
      "outputs": [],
      "source": [
        "# Get indices per class\n",
        "manipulated_idx = torch.where(train_labels == 0)[0]  # manipulated\n",
        "real_idx        = torch.where(train_labels == 1)[0]  # real"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOTX1xaSDmkU"
      },
      "outputs": [],
      "source": [
        "print(manipulated_idx)\n",
        "print(real_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7QfonPwBRhB"
      },
      "outputs": [],
      "source": [
        "target_size = min(len(manipulated_idx), 20000)  # cap manipulated to ~20k\n",
        "# Oversample real -> up to target_size\n",
        "repeat_factor = target_size // len(real_idx)\n",
        "remainder = target_size % len(real_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMi9ZyvbBVml"
      },
      "outputs": [],
      "source": [
        "real_idx_oversampled = real_idx.repeat(repeat_factor)\n",
        "if remainder > 0:\n",
        "    real_idx_oversampled = torch.cat([\n",
        "        real_idx_oversampled,\n",
        "        real_idx[torch.randperm(len(real_idx))[:remainder]]\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZuQiw-uO8tI"
      },
      "outputs": [],
      "source": [
        "# Undersample manipulated -> down to target_size\n",
        "manipulated_idx_sampled = manipulated_idx[torch.randperm(len(manipulated_idx))[:target_size]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVd87Ol7PPY1"
      },
      "source": [
        "# -----------------------------\n",
        "# Combine both balanced sets\n",
        "# -----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoh_wBl8BZjy"
      },
      "outputs": [],
      "source": [
        "epoch_indices = torch.cat([manipulated_idx_sampled, real_idx_oversampled])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i_Lbrq8Bd5S"
      },
      "outputs": [],
      "source": [
        "# Shuffle\n",
        "epoch_indices = epoch_indices[torch.randperm(len(epoch_indices))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1UYRrQYBgin"
      },
      "source": [
        "# --------------------------\n",
        "# DataLoaders\n",
        "# --------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7FqpNtaBmL9"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          sampler=SubsetRandomSampler(epoch_indices),\n",
        "                          num_workers=2)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                        shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvgydgbXfpoM"
      },
      "outputs": [],
      "source": [
        "# Class label map\n",
        "label_map = full_dataset.class_to_idx\n",
        "print(\"‚úÖ Classes:\", label_map)\n",
        "print(f\"üìÇ Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKNSI0PpXnKM"
      },
      "outputs": [],
      "source": [
        "# print(f\"üì¶ Number of batches in train_loader: {len(train_loader)}\")\n",
        "# print(f\"üì¶ Number of batches in val_loader: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTICBZakETk9"
      },
      "outputs": [],
      "source": [
        "print(f\"üì¶ Number of batches in train_loader: {len(train_loader)}\")\n",
        "print(f\"üì¶ Number of batches in val_loader: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yp-d_1sWJSD"
      },
      "source": [
        "## Load ViT and train the model and Save the best model at drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsbGVk0p8c4E"
      },
      "outputs": [],
      "source": [
        "save_path = '/content/drive/MyDrive/ModelTrain/DFD/Model/best_vit_model.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U385HMqtpRHS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "\n",
        "class HybridXceptionViT(nn.Module):\n",
        "    def __init__(self, num_classes=2, freeze_backbones=False):\n",
        "        super(HybridXceptionViT, self).__init__()\n",
        "\n",
        "        # -----------------------------\n",
        "        # Xception backbone (CNN)\n",
        "        # -----------------------------\n",
        "        self.xception = timm.create_model('xception', pretrained=True, num_classes=0)\n",
        "        self.xception_fc_dim = self.xception.num_features  # usually 2048\n",
        "\n",
        "        # -----------------------------\n",
        "        # ViT backbone (Transformer)\n",
        "        # -----------------------------\n",
        "        self.vit = timm.create_model('vit_large_patch16_224.orig_in21k', pretrained=True, num_classes=0)\n",
        "        self.vit_fc_dim = self.vit.num_features  # usually 1024 or 1280\n",
        "\n",
        "        # -----------------------------\n",
        "        # Optionally freeze backbones\n",
        "        # -----------------------------\n",
        "        if freeze_backbones:\n",
        "            for param in self.xception.parameters():\n",
        "                param.requires_grad = False\n",
        "            for param in self.vit.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # -----------------------------\n",
        "        # Fully connected classification head\n",
        "        # -----------------------------\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.xception_fc_dim + self.vit_fc_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # -----------------------------\n",
        "        # Resize input for each backbone\n",
        "        # -----------------------------\n",
        "        x_xception = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "        x_vit = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "\n",
        "\n",
        "        # -----------------------------\n",
        "        # Extract features\n",
        "        # -----------------------------\n",
        "        x_feat = self.xception(x_xception)  # (batch_size, 2048)\n",
        "        vit_feat = self.vit(x_vit)          # (batch_size, 1024)\n",
        "\n",
        "        # -----------------------------\n",
        "        # Concatenate features\n",
        "        # -----------------------------\n",
        "        combined = torch.cat([x_feat, vit_feat], dim=1)  # (batch_size, 3072)\n",
        "\n",
        "        # -----------------------------\n",
        "        # Classification head\n",
        "        # -----------------------------\n",
        "        out = self.fc(combined)  # (batch_size, num_classes)\n",
        "        return out\n",
        "\n",
        "# -----------------------------\n",
        "# Initialize model\n",
        "# -----------------------------\n",
        "model = HybridXceptionViT(num_classes=2, freeze_backbones=False).to(device)\n",
        "\n",
        "print(\"‚úÖ HybridXceptionViT model initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4PDIc0d1TEV"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# compute weights based on dataset distribution\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(labels),\n",
        "    y=labels\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR5zdr62Sh7y"
      },
      "outputs": [],
      "source": [
        "# # Example counts\n",
        "# manipulated_count = 9647\n",
        "# real_count = 1350\n",
        "# class_counts = [manipulated_count, real_count]  # [class0, class1]\n",
        "\n",
        "# # Inverse frequency\n",
        "# weights = [sum(class_counts) / c for c in class_counts]\n",
        "\n",
        "# class_weights = torch.tensor(weights, dtype=torch.float).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0IQ0jJMP-rO"
      },
      "outputs": [],
      "source": [
        "# # Load Vision Transformer (ViT) model\n",
        "# model = timm.create_model('vit_large_patch16_224', pretrained=True, num_classes=2)\n",
        "# model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AybPplJfWORz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# =====================\n",
        "# Loss, Optimizer, Scheduler\n",
        "# =====================\n",
        "# If your dataset is imbalanced, use class_weights; otherwise just CrossEntropyLoss()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "# =====================\n",
        "# Training Settings\n",
        "# =====================\n",
        "num_epochs = 10\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "# Metric storage\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "# Reverse label map\n",
        "idx_to_class = {v: k for k, v in label_map.items()}\n",
        "\n",
        "# =====================\n",
        "# Checkpoints\n",
        "# =====================\n",
        "checkpoint_dir = \"/content/drive/MyDrive/ModelTrain/DFD/checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "latest_ckpt = os.path.join(checkpoint_dir, \"last_checkpoint.pth\")\n",
        "\n",
        "# =====================\n",
        "# Resume Training if Checkpoint Exists\n",
        "# =====================\n",
        "start_epoch = 0\n",
        "best_val_accuracy = 0.0\n",
        "\n",
        "if os.path.exists(latest_ckpt):\n",
        "    checkpoint = torch.load(latest_ckpt, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
        "    start_epoch = checkpoint[\"epoch\"] + 1\n",
        "    best_val_accuracy = checkpoint[\"best_val_accuracy\"]\n",
        "\n",
        "    # restore history lists\n",
        "    train_losses = checkpoint.get(\"train_losses\", [])\n",
        "    val_losses = checkpoint.get(\"val_losses\", [])\n",
        "    train_accuracies = checkpoint.get(\"train_accuracies\", [])\n",
        "    val_accuracies = checkpoint.get(\"val_accuracies\", [])\n",
        "\n",
        "    print(f\"‚úÖ Resuming training from epoch {start_epoch}\")\n",
        "    print(f\"üìä Previous best validation accuracy: {best_val_accuracy:.2f}%\")\n",
        "\n",
        "    if start_epoch > 0:\n",
        "        print(\"\\nüìã Previous Training History:\")\n",
        "        print(\"Epoch | Train Loss | Val Loss | Train Acc | Val Acc\")\n",
        "        for i in range(start_epoch):\n",
        "            print(f\"{i+1:5d} | {train_losses[i]:10.4f} | {val_losses[i]:8.4f} | \"\n",
        "                  f\"{train_accuracies[i]:9.2f}% | {val_accuracies[i]:7.2f}%\")\n",
        "\n",
        "# =====================\n",
        "# Training Loop\n",
        "# =====================\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(f\"\\nüéØ Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # ----- Training -----\n",
        "    model.train()\n",
        "    running_loss, correct_train, total_train = 0.0, 0, 0\n",
        "    for images, labels in tqdm(train_loader, desc=\"Training\", unit=\"batch\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    # ----- Validation -----\n",
        "    model.eval()\n",
        "    val_running_loss, correct_val, total_val = 0.0, 0, 0\n",
        "    true_labels, pred_labels = [], []\n",
        "\n",
        "    for images, labels in tqdm(val_loader, desc=\"Validating\", unit=\"batch\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            pred_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_running_loss / len(val_loader)\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # ----- Print Epoch Metrics -----\n",
        "    print(f\"\\nüìâ Train Loss: {avg_train_loss:.4f} | üìà Val Loss: {avg_val_loss:.4f} | \"\n",
        "          f\"‚úÖ Train Acc: {train_acc:.2f}% | ‚úÖ Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    # ----- Scheduler Step -----\n",
        "    scheduler.step()\n",
        "    print(f\"üìâ Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # ----- Save Best Model -----\n",
        "    if val_acc > best_val_accuracy:\n",
        "        best_val_accuracy = val_acc\n",
        "        torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"best_model.pth\"))\n",
        "        patience_counter = 0\n",
        "        print(f\"üåü Best model updated! Val Acc: {best_val_accuracy:.2f}%\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"‚ö†Ô∏è No improvement. Best so far: {best_val_accuracy:.2f}%\")\n",
        "        if patience_counter >= patience:\n",
        "            print(\"‚èπÔ∏è Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "    # ----- Save Latest Checkpoint (AFTER updating best_val_accuracy) -----\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "        \"loss\": avg_val_loss,\n",
        "        \"best_val_accuracy\": best_val_accuracy,  # now always correct\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_losses\": val_losses,\n",
        "        \"train_accuracies\": train_accuracies,\n",
        "        \"val_accuracies\": val_accuracies\n",
        "    }, latest_ckpt)\n",
        "\n",
        "    # Optional: save epoch-specific checkpoint\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
        "        \"loss\": avg_val_loss,\n",
        "        \"val_accuracy\": val_acc\n",
        "    }, os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\"))\n",
        "\n",
        "    print(f\"üíæ Saved latest checkpoint for epoch {epoch}\")\n",
        "\n",
        "    # ----- Classification Report -----\n",
        "    print(\"\\nüßæ Classification Report:\")\n",
        "    print(classification_report(\n",
        "        [idx_to_class[i] for i in true_labels],\n",
        "        [idx_to_class[i] for i in pred_labels],\n",
        "        digits=3\n",
        "    ))\n",
        "\n",
        "print(f\"\\nüéØ Training completed. Best Validation Accuracy: {best_val_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1mhm9pFynaC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ----- 1. Load model checkpoint -----\n",
        "checkpoint_path = \"/content/drive/MyDrive/ModelTrain/DFD/checkpoints/best_model.pth\"  # change path\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# ----- 2. Validation loop -----\n",
        "val_running_loss, correct_val, total_val = 0.0, 0, 0\n",
        "true_labels, pred_labels = [], []\n",
        "\n",
        "for images, labels in tqdm(val_loader, desc=\"Validating\", unit=\"batch\"):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total_val += labels.size(0)\n",
        "        correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        pred_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "avg_val_loss = val_running_loss / len(val_loader)\n",
        "val_acc = 100 * correct_val / total_val\n",
        "\n",
        "print(f\"\\nüìâ Validation Loss: {avg_val_loss:.4f} | ‚úÖ Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "# ----- 3. Optional: Classification Report -----\n",
        "idx_to_class = {v: k for k, v in label_map.items()}  # if needed\n",
        "print(\"\\nüßæ Classification Report:\")\n",
        "print(classification_report(\n",
        "    [idx_to_class[i] for i in true_labels],\n",
        "    [idx_to_class[i] for i in pred_labels],\n",
        "    digits=3\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFSrSMfPtP0H"
      },
      "source": [
        "## Accuracy Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ffgkfom4g2xr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =====================\n",
        "# Plot Loss & Accuracy Curves\n",
        "# =====================\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# ----- Loss Curve -----\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
        "plt.plot(epochs, val_losses, 'r-', label='Val Loss')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# ----- Accuracy Curve -----\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs, train_accuracies, 'b-', label='Train Accuracy')\n",
        "plt.plot(epochs, val_accuracies, 'g-', label='Val Accuracy')\n",
        "plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4JfnMYztSUP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Print Validation Loss and Accuracy\n",
        "print(f\"üìà Val Loss: {avg_val_loss:.4f}\")\n",
        "print(f\"‚úÖ Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nüßæ Classification Report:\")\n",
        "print(classification_report(\n",
        "    [idx_to_class[i] for i in true_labels],\n",
        "    [idx_to_class[i] for i in pred_labels],\n",
        "    digits=3\n",
        "))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Manipulated', 'Real'], yticklabels=['Manipulated', 'Real'])\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM85PzeV8GTO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Print Validation Loss and Accuracy\n",
        "print(f\"üìà Val Loss: {avg_val_loss:.4f}\")\n",
        "print(f\"‚úÖ Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nüßæ Classification Report:\")\n",
        "print(classification_report(\n",
        "    [idx_to_class[i] for i in true_labels],\n",
        "    [idx_to_class[i] for i in pred_labels],\n",
        "    digits=3\n",
        "))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# Normalize (optional, for percentages)\n",
        "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Labels from idx_to_class\n",
        "labels = [idx_to_class[i] for i in sorted(idx_to_class.keys())]\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=labels, yticklabels=labels)\n",
        "\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}